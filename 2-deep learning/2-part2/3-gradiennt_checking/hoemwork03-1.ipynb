{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>Gradient Checking——学习如何实现和使用梯度检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # 导入numpy库，用于数学和数组操作\n",
    "from testCases import *  # 导入预定义的测试用例\n",
    "from gc_utils import sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector  # 导入自定义的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x, theta):\n",
    "    \"\"\"\n",
    "     实现图 1 中所示的线性前向传播（计算 J）(J(theta) = theta * x)\n",
    "    \n",
    "     参数：\n",
    "     x -- 实数值输入\n",
    "     theta——我们的参数，也是一个实数\n",
    "    \n",
    "     返回值：\n",
    "     J -- 函数 J 的值，使用公式 J(theta) = theta * x 计算\n",
    "    \"\"\"\n",
    "    \n",
    "    J = np.dot(theta, x) # 执行前向传播计算，计算预测值 J\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J = 8\n"
     ]
    }
   ],
   "source": [
    "x, theta = 2, 4  # 设置输入特征 x 和参数 theta 的值\n",
    "J = forward_propagation(x, theta)  # 调用 forward_propagation 函数进行前向传播计算\n",
    "print(\"J = \" + str(J))  # 输出预测值 J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(x, theta):\n",
    "    \"\"\"\n",
    "    计算 J 相对于 theta 的导数（参见图 1）。\n",
    "    \n",
    "     参数：\n",
    "     x -- 实数值输入\n",
    "     theta——我们的参数，也是一个实数\n",
    "    \n",
    "     返回值：\n",
    "     dtheta -- 成本相对于 theta 的梯度\n",
    "    \"\"\"\n",
    "    \n",
    "    dtheta = x  # 计算参数 theta 的梯度\n",
    "    \n",
    "    return dtheta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtheta = 2\n"
     ]
    }
   ],
   "source": [
    "x, theta = 2, 4  # 初始化输入特征 x 和参数 theta 的值\n",
    "dtheta = backward_propagation(x, theta)  # 调用 backward_propagation 函数计算参数 theta 的梯度\n",
    "print (\"dtheta = \" + str(dtheta))  # 打印输出参数 theta 的梯度值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(x, theta, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    实施图 1 中所示的反向传播。\n",
    "    \n",
    "     参数：\n",
    "     x -- 实数值输入\n",
    "     theta——我们的参数，也是一个实数\n",
    "     epsilon -- 对输入的微小偏移以使用公式 (1) 计算近似梯度\n",
    "    \n",
    "     返回值：\n",
    "     difference -- 近似梯度与后向传播梯度的差（2）\n",
    "    \"\"\"\n",
    "    \n",
    "    thetaplus = theta + epsilon  # 在参数 theta 上加上一个极小的值 epsilon\n",
    "    thetaminus = theta - epsilon  # 在参数 theta 上减去一个极小的值 epsilon\n",
    "    J_plus = forward_propagation(x, thetaplus)  # 使用更新后的 theta 计算正向传播的损失函数值 J\n",
    "    J_minus = forward_propagation(x, thetaminus)  # 使用更新后的 theta 计算正向传播的损失函数值 J\n",
    "    gradapprox = (J_plus - J_minus) / (2 * epsilon)  # 使用中心差分法计算梯度的近似值 gradapprox\n",
    "    \n",
    "    grad = backward_propagation(x, theta)  # 使用反向传播计算真实的梯度值 grad\n",
    "    \n",
    "    numerator = np.linalg.norm(grad - gradapprox)  # 计算梯度差的范数作为分子\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)  # 计算两个梯度的范数之和作为分母\n",
    "    difference = numerator / denominator  # 计算梯度差的相对差异度\n",
    "    \n",
    "    if difference < 1e-7:  # 判断梯度差的相对差异度是否小于阈值\n",
    "        print(\"The gradient is correct!\")  # 如果相对差异度小于阈值，则梯度正确\n",
    "    else:\n",
    "        print(\"The gradient is wrong!\")  # 如果相对差异度大于等于阈值，则梯度错误\n",
    "    \n",
    "    return difference  # 返回梯度差的相对差异度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient is correct!\n",
      "difference = 2.919335883291695e-10\n"
     ]
    }
   ],
   "source": [
    "x, theta = 2, 4  # 定义输入 x 和参数 theta 的值\n",
    "difference = gradient_check(x, theta)  # 调用梯度检查函数，计算梯度差的相对差异度\n",
    "print(\"difference = \" + str(difference))  # 打印梯度差的相对差异度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_n(X, Y, parameters):\n",
    "    \"\"\"\n",
    "    实现图 3 中所示的前向传播（并计算成本）。\n",
    "    \n",
    "     参数：\n",
    "     X——m 个例子的训练集\n",
    "     Y——m 个例子的标签\n",
    "     参数——包含你的参数“W1”、“b1”、“W2”、“b2”、“W3”、“b3”的 python 字典：\n",
    "     W1 -- 形状的权重矩阵 (5, 4)\n",
    "     b1 -- 形状 (5, 1) 的偏置向量\n",
    "     W2 -- 形状为 (3, 5) 的权重矩阵\n",
    "     b2 -- 形状 (3, 1) 的偏置向量\n",
    "     W3 -- 形状为 (1, 3) 的权重矩阵\n",
    "     b3 -- 形状为 (1, 1) 的偏置向量\n",
    "    \n",
    "     返回值：\n",
    "     成本——成本函数（例如物流成本）\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]  # 获取训练样本数量\n",
    "    W1 = parameters[\"W1\"]  # 获取参数 W1\n",
    "    b1 = parameters[\"b1\"]  # 获取参数 b1\n",
    "    W2 = parameters[\"W2\"]  # 获取参数 W2\n",
    "    b2 = parameters[\"b2\"]  # 获取参数 b2\n",
    "    W3 = parameters[\"W3\"]  # 获取参数 W3\n",
    "    b3 = parameters[\"b3\"]  # 获取参数 b3\n",
    "\n",
    "    Z1 = np.dot(W1, X) + b1  # 计算第一层的线性输出 Z1\n",
    "    A1 = relu(Z1)  # 对 Z1 进行激活函数 relu 操作，得到第一层的激活值 A1\n",
    "    Z2 = np.dot(W2, A1) + b2  # 计算第二层的线性输出 Z2\n",
    "    A2 = relu(Z2)  # 对 Z2 进行激活函数 relu 操作，得到第二层的激活值 A2\n",
    "    Z3 = np.dot(W3, A2) + b3  # 计算第三层的线性输出 Z3\n",
    "    A3 = sigmoid(Z3)  # 对 Z3 进行激活函数 sigmoid 操作，得到第三层的激活值 A3\n",
    "\n",
    "    logprobs = np.multiply(-np.log(A3), Y) + np.multiply(-np.log(1 - A3), 1 - Y)  # 计算交叉熵损失函数\n",
    "    cost = 1. / m * np.sum(logprobs)  # 计算平均损失\n",
    "    \n",
    "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)  # 将中间变量保存到缓存中\n",
    "    \n",
    "    return cost, cache  # 返回损失和缓存\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_n(X, Y, cache):\n",
    "    \"\"\"\n",
    "    实施图 2 中所示的反向传播。\n",
    "    \n",
    "     参数：\n",
    "     X -- 输入数据点，形状（输入大小，1）\n",
    "     Y——真正的“标签”\n",
    "     cache -- 缓存 forward_propagation_n() 的输出\n",
    "    \n",
    "     返回值：\n",
    "     gradients -- 一个字典，其中包含关于每个参数、激活和预激活变量的成本梯度。\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]  # 获取训练样本数量\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache  # 从缓存中获取中间变量\n",
    "    \n",
    "    dZ3 = A3 - Y  # 计算第三层的梯度\n",
    "    dW3 = 1. / m * np.dot(dZ3, A2.T)  # 计算第三层权重的梯度\n",
    "    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)  # 计算第三层偏置的梯度\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)  # 计算第二层的激活值的梯度\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))  # 计算第二层的线性输出的梯度\n",
    "    dW2 = 1. / m * np.dot(dZ2, A1.T) * 2  # 计算第二层权重的梯度\n",
    "    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)  # 计算第二层偏置的梯度\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)  # 计算第一层的激活值的梯度\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))  # 计算第一层的线性输出的梯度\n",
    "    dW1 = 1. / m * np.dot(dZ1, X.T)  # 计算第一层权重的梯度\n",
    "    db1 = 4. / m * np.sum(dZ1, axis=1, keepdims=True)  # 计算第一层偏置的梯度\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}  # 将梯度保存到字典中\n",
    "    \n",
    "    return gradients  # 返回梯度字典\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check_n(parameters, gradients, X, Y, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    检查 backward_propagation_n 是否正确计算了 forward_propagation_n 输出的成本梯度\n",
    "    \n",
    "     参数：\n",
    "     参数——包含你的参数“W1”、“b1”、“W2”、“b2”、“W3”、“b3”的 python 字典：\n",
    "     grad——backward_propagation_n 的输出，包含成本相对于参数的梯度。\n",
    "     x -- 输入数据点，形状（输入大小，1）\n",
    "     y -- 真正的“标签”\n",
    "     epsilon -- 对输入的微小偏移以使用公式 (1) 计算近似梯度\n",
    "    \n",
    "     返回值：\n",
    "     difference -- 近似梯度与后向传播梯度的差（2）\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters_values, _ = dictionary_to_vector(parameters)  # 将参数字典转换为向量\n",
    "    grad = gradients_to_vector(gradients)  # 将梯度字典转换为向量\n",
    "    num_parameters = parameters_values.shape[0]  # 获取参数的数量\n",
    "    J_plus = np.zeros((num_parameters, 1))  # 创建存储正向传播误差的数组\n",
    "    J_minus = np.zeros((num_parameters, 1))  # 创建存储反向传播误差的数组\n",
    "    gradapprox = np.zeros((num_parameters, 1))  # 创建存储梯度近似值的数组\n",
    "    \n",
    "    for i in range(num_parameters):\n",
    "        thetaplus = np.copy(parameters_values)  # 复制参数向量\n",
    "        thetaplus[i][0] = thetaplus[i][0] + epsilon  # 增加 epsilon\n",
    "        J_plus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaplus))  # 正向传播，计算正向误差\n",
    "        \n",
    "        thetaminus = np.copy(parameters_values)  # 复制参数向量\n",
    "        thetaminus[i][0] = thetaminus[i][0] - epsilon  # 减少 epsilon\n",
    "        J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaminus))  # 正向传播，计算反向误差\n",
    "        \n",
    "        gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)  # 计算梯度的近似值\n",
    "    \n",
    "    numerator = np.linalg.norm(grad - gradapprox)  # 计算差异的分子\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)  # 计算差异的分母\n",
    "    difference = numerator / denominator  # 计算差异\n",
    "    \n",
    "    if difference > 1e-7:\n",
    "        print(\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    else:\n",
    "        print(\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    \n",
    "    return difference  # 返回差异值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mThere is a mistake in the backward propagation! difference = 0.2850931566540251\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "X, Y, parameters = gradient_check_n_test_case()  # 获取测试数据\n",
    "\n",
    "cost, cache = forward_propagation_n(X, Y, parameters)  # 进行正向传播计算损失和缓存\n",
    "gradients = backward_propagation_n(X, Y, cache)  # 进行反向传播计算梯度\n",
    "difference = gradient_check_n(parameters, gradients, X, Y)  # 进行梯度检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
